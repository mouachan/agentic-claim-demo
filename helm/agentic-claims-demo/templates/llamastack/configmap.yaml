{{- if and .Values.llamastack.enabled }}
# RHOAI3.0, running llamastack v0.3.0rc3+rhai0
---
apiVersion: v1
kind: ConfigMap
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "3" # after inferenceservice
  labels:
    {{- include "agentic-claims-demo.labels" . | nindent 4 }}
    app: llamastack-v030-rc3
  name: llamastack-rhoai
  namespace: {{ include "agentic-claims-demo.namespace" . }}
data:
  run.yaml: |
    version: "2"
    image_name: rh

    apis:
    - agents
    # "batches" is missing from original LlamaStackDistribution copy
    - batches
    - datasetio
    - eval
    - files
    - inference
    - safety
    - scoring
    # "telemetry" was missing from demo/present in LlamaStackDistribution copy
    # - telemetry
    - tool_runtime
    - vector_io

    providers:
      inference:
        - provider_id: sentence-transformers
          provider_type: inline::sentence-transformers
          config: {}

        - provider_id: vllm-inference-1
          provider_type: remote::vllm
          config:
            api_token: ${env.VLLM_API_TOKEN:=fake}
            max_tokens: ${env.VLLM_MAX_TOKENS:=16384}
            tls_verify: ${env.VLLM_TLS_VERIFY:=false}
            url: ${env.VLLM_URL}

        - provider_id: gemma-embedding
          provider_type: remote::vllm
          config:
            api_token: ${env.GEMMA_API_TOKEN:=fake}
            max_tokens: ${env.GEMMA_MAX_TOKENS:=512}
            tls_verify: ${env.GEMMA_TLS_VERIFY:=false}
            url: ${env.GEMMA_URL}

      vector_io:
        - provider_id: pgvector
          provider_type: remote::pgvector
          # looks valid? as per
          # https://github.com/llamastack/llama-stack/blob/589c0d005b826c30409d75935f860a109a038e31/src/llama_stack/distributions/open-benchmark/config.yaml#L52-L62
          # then again, agent configs mismatches
          config:
            host: ${env.POSTGRES_HOST:=localhost}
            port: ${env.POSTGRES_PORT:=5432}
            db: ${env.POSTGRES_DATABASE:=claims_db}
            user: ${env.POSTGRES_USER:=postgres}
            password: ${env.POSTGRES_PASSWORD}
            # INFO     2026-01-09 13:12:21,454 llama_stack.providers.remote.vector_io.pgvector.pgvector:360 vector_io::pgvector:
            #          Initializing PGVector memory adapter with config: host='postgresql' port=5432 db='democlaimsdb'
            #          user='<cleartextusername>' password='<cleartextpassword... good to know it was properly read...>' kvstore=None
            # Traceback (most recent call last):
            # [...]
            #   File "/opt/app-root/lib64/python3.12/site-packages/llama_stack/providers/remote/vector_io/pgvector/pgvector.py", line 361, in initialize
            #     self.kvstore = await kvstore_impl(self.config.kvstore)
            #                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            #   File "/opt/app-root/lib64/python3.12/site-packages/llama_stack/providers/utils/kvstore/kvstore.py", line 48, in kvstore_impl
            #     if config.type == KVStoreType.redis.value:
            #        ^^^^^^^^^^^
            # AttributeError: 'NoneType' object has no attribute 'type'

            # => digging into image ... llamastack 0.3.0-rc3
            # /opt/app-root/lib/python3.12/site-packages/llama_stack/distributions/postgres-demo/run.yaml
            # doesn't tell about "persistence".
            # but does mention a kvstore, with type=sqlite (despite having postgres coordinates above)
            kvstore:
              type: sqlite
              # could we have type=postgres? and re-define credentials?
              # llama_stack/providers/providers/utils/kvstore/kvstore.py would allow for this ... although quite abstract at that stage
              db_path: /opt/app-root/src/.llama/distributions/rh/pgvector_registry.db

      # missing from demo/present in default RHOAI config
      # safety:
      #   - provider_id: trustyai_fms
      #     provider_type: remote::trustyai_fms
      #     module: llama_stack_provider_trustyai_fms==0.2.2
      #     config:
      #       orchestrator_url: ${env.FMS_ORCHESTRATOR_URL:=}
      #       ssl_cert_path: ${env.FMS_SSL_CERT_PATH:=}
      #       shields: {}

      agents:
        - provider_id: meta-reference
          provider_type: inline::meta-reference
          # adjusting from demo, taking from
          # https://github.com/llamastack/llama-stack/blob/589c0d005b826c30409d75935f860a109a038e31/benchmarking/k8s-benchmark/stack-configmap.yaml#L51-L68
          config:
            persistence_store:
              type: postgres
              host: ${env.POSTGRES_HOST:=localhost}
              port: ${env.POSTGRES_PORT:=5432}
              db: ${env.POSTGRES_DATABASE:=claims_db}
              user: ${env.POSTGRES_USER:=postgres}
              password: ${env.POSTGRES_PASSWORD}
            responses_store:
              type: postgres
              host: ${env.POSTGRES_HOST:=localhost}
              port: ${env.POSTGRES_PORT:=5432}
              db: ${env.POSTGRES_DATABASE:=claims_db}
              user: ${env.POSTGRES_USER:=postgres}
              password: ${env.POSTGRES_PASSWORD}

      tool_runtime:
        - provider_id: rag-runtime
          provider_type: inline::rag-runtime
          config: {}
        - provider_id: model-context-protocol
          provider_type: remote::model-context-protocol
          config: {}
        # RHOAI ALSO had
        # - provider_id: brave-search
        #   provider_type: remote::brave-search
        #   config:
        #     api_key: ${env.BRAVE_SEARCH_API_KEY:=}
        #     max_results: 3
        # - provider_id: tavily-search
        #   provider_type: remote::tavily-search
        #   config:
        #     api_key: ${env.TAVILY_SEARCH_API_KEY:=}
        #     max_results: 3

      files:
        - provider_id: meta-reference-files
          provider_type: inline::localfs
          # RHOAI had
          # config:
          #   storage_dir: /opt/app-root/src/.llama/distributions/rh/files
          #   metadata_store:
          #     type: sqlite
          #     db_path: /opt/app-root/src/.llama/distributions/rh/files_metadata.db
          config:
            storage_dir: /opt/app-root/src/.llama/distributions/rh/files
            # pydantic_core._pydantic_core.ValidationError: 1 validation error for LocalfsFilesImplConfig
            #  Unable to extract tag using discriminator 'type' [type=union_tag_not_found, input_value={'backend': 'sql_default'...name': 'files_metadata'}, input_type=dict]
            #    For further information visit https://errors.pydantic.dev/2.12/v/union_tag_not_found
            # metadata_store:
            #   backend: sql_default
            #   table_name: files_metadata
            # => keeping RHOAI config, then ...
            metadata_store:
              type: sqlite
              db_path: /opt/app-root/src/.llama/distributions/rh/files_metadata.db

      # missing from RHOAI?
      batches:
        - provider_id: reference
          provider_type: inline::reference
          config:
            kvstore:
              # pydantic_core._pydantic_core.ValidationError: 1 validation error for ReferenceBatchesImplConfig
              # kvstore
              #   Unable to extract tag using discriminator 'type' [type=union_tag_not_found, input_value={'namespace': 'batches', 'backend': 'kv_default'}, input_type=dict]
              #     For further information visit https://errors.pydantic.dev/2.12/v/union_tag_not_found
              # namespace: batches
              # backend: kv_default
              # => duplicating llama_stack-distributions/starter-gpu/run.yaml
              type: sqlite
              db_path: /opt/app-root/src/.llama/distributions/rh/batches.db
      eval:
        - provider_id: meta-reference-eval
          provider_type: inline::meta-reference
          # https://github.com/llamastack/llama-stack/blob/589c0d005b826c30409d75935f860a109a038e31/src/llama_stack/distributions/dell/run-with-safety.yaml#L50-L56
          config:
            kvstore:
              # => MetaReferenceEvalConfig
              # backend: kv_default
              # namespace: eval
              # => let's try ...
              type: postgres
              host: ${env.POSTGRES_HOST:=localhost}
              port: ${env.POSTGRES_PORT:=5432}
              db: ${env.POSTGRES_DATABASE:=claims_db}
              user: ${env.POSTGRES_USER:=postgres}
              password: ${env.POSTGRES_PASSWORD}
        # RHOAI had:
        # - provider_id: trustyai_lmeval
        #   provider_type: remote::trustyai_lmeval
        #   module: llama_stack_provider_lmeval==0.2.4
        #   config:
        #       use_k8s: ${env.TRUSTYAI_LMEVAL_USE_K8S:=true}
        #       base_url: ${env.VLLM_URL:=http://localhost:8000/v1}
        # - provider_id: ${env.EMBEDDING_MODEL:+trustyai_ragas_inline}
        #   provider_type: inline::trustyai_ragas
        #   module: llama_stack_provider_ragas.inline
        #   config:
        #     embedding_model: ${env.EMBEDDING_MODEL:=}
        # - provider_id: ${env.KUBEFLOW_LLAMA_STACK_URL:+trustyai_ragas_remote}
        #   provider_type: remote::trustyai_ragas
        #   module: llama_stack_provider_ragas.remote
        #   config:
        #     embedding_model: ${env.EMBEDDING_MODEL:=}
        #     kubeflow_config:
        #       results_s3_prefix: ${env.KUBEFLOW_RESULTS_S3_PREFIX:=}
        #       s3_credentials_secret_name: ${env.KUBEFLOW_S3_CREDENTIALS_SECRET_NAME:=}
        #       pipelines_endpoint: ${env.KUBEFLOW_PIPELINES_ENDPOINT:=}
        #       namespace: ${env.KUBEFLOW_NAMESPACE:=}
        #       llama_stack_url: ${env.KUBEFLOW_LLAMA_STACK_URL:=}
        #       base_image: ${env.KUBEFLOW_BASE_IMAGE:=}
        #       pipelines_api_token: ${env.KUBEFLOW_PIPELINES_TOKEN:=}

      datasetio:
        - provider_id: huggingface
          provider_type: remote::huggingface
          config:
            kvstore:
              # pydantic_core._pydantic_core.ValidationError: 1 validation error for HuggingfaceDatasetIOConfig
              # kvstore
              #   Unable to extract tag using discriminator 'type' [type=union_tag_not_found, input_value={'backend': 'kv_default',...datasetio::huggingface'}, input_type=dict]
              #     For further information visit https://errors.pydantic.dev/2.12/v/union_tag_not_found
              # backend: kv_default
              # namespace: datasetio::huggingface
              # => llama_stack-distributions/starter-gpu/run.yaml
              type: sqlite
              db_path: /opt/app-root/src/.llama/distributions/rh/huggingface_datasetio.db
        # RHOAI had:
        # - provider_id: huggingface
        #   provider_type: remote::huggingface
        #   config:
        #     kvstore:
        #       type: sqlite
        #       namespace: null
        #       db_path: /opt/app-root/src/.llama/distributions/rh/huggingface_datasetio.db
        # - provider_id: localfs
        #   provider_type: inline::localfs
        #   config:
        #     kvstore:
        #       type: sqlite
        #       namespace: null
        #       db_path: /opt/app-root/src/.llama/distributions/rh/localfs_datasetio.db

      scoring:
        - provider_id: basic
          provider_type: inline::basic
          config: {}
        - provider_id: llm-as-judge
          provider_type: inline::llm-as-judge
          config: {}
        # RHOAI ALSO had:
        # - provider_id: braintrust
        #   provider_type: inline::braintrust
        #   config:
        #     openai_api_key: ${env.OPENAI_API_KEY:=}

      # RHOAI had:
      # telemetry:
      # - provider_id: meta-reference
      #   provider_type: inline::meta-reference
      #   config:
      #     service_name: "${env.OTEL_SERVICE_NAME:=\u200B}"
      #     sinks: ${env.TELEMETRY_SINKS:=console}
      #     otel_exporter_otlp_endpoint: ${env.OTEL_EXPORTER_OTLP_ENDPOINT:=}

    # RHOAI had:
    # metadata_store:
    #   type: sqlite
    #   db_path: /opt/app-root/src/.llama/distributions/rh/registry.db
    # inference_store:
    #   type: sqlite
    #   db_path: /opt/app-root/src/.llama/distributions/rh/inference_store.db

    # for certain: that block is not processed. when starting, llamastack prints back
    # its configuration. There's no storage block, no backends, kv_default, sql_default, ...
    # storage:
    #   backends:
    #     kv_default:
    #       # checking everywhere in their code: kv_postgres, or sql_postgres, do not exist
    #       # kvstore type is redis/mongodb/postgres/sqlite, no prefixes
    #       # is this even used though? didn't find this in config samples
    #       # for 0.3.0-rc3
    #       type: kv_postgres
    #       host: ${env.POSTGRES_HOST:=localhost}
    #       port: ${env.POSTGRES_PORT:=5432}
    #       db: ${env.POSTGRES_DATABASE:=claims_db}
    #       user: ${env.POSTGRES_USER:=postgres}
    #       password: ${env.POSTGRES_PASSWORD}
    #       table_name: llamastack_kvstore
    #     sql_default:
    #       type: sql_postgres
    #       host: ${env.POSTGRES_HOST:=localhost}
    #       port: ${env.POSTGRES_PORT:=5432}
    #       db: ${env.POSTGRES_DATABASE:=claims_db}
    #       user: ${env.POSTGRES_USER:=postgres}
    #       password: ${env.POSTGRES_PASSWORD}
    #   stores:
    #     metadata:
    #       backend: kv_default
    #       namespace: registry
    #     inference:
    #       table_name: inference_store
    #       backend: sql_default
    #       max_write_queue_size: 10000
    #       num_writers: 4
    #     conversations:
    #       table_name: openai_conversations
    #       backend: sql_default
    #     prompts:
    #       namespace: prompts
    #       backend: kv_default

    models:
      - metadata: {}
        model_id: ${env.INFERENCE_MODEL}
        provider_id: vllm-inference-1
        # demo had/missing from RHOAI
        # provider_model_id: ${env.INFERENCE_MODEL}
        model_type: llm

      - metadata:
          embedding_dimension: {{ .Values.llamastack.embedding.dimension }}
        model_id: ${env.EMBEDDING_MODEL}
        provider_id: gemma-embedding
        # demo had/missing from RHOAI
        # provider_model_id: ${env.EMBEDDING_MODEL}
        model_type: embedding

    shields: []

    # was empty in RHOAI default config
    vector_dbs:
      - vector_db_id: claims_vector_db
        provider_id: pgvector
        embedding_model: ${env.EMBEDDING_MODEL}
        embedding_dimension: {{ .Values.llamastack.embedding.dimension }}

    datasets: []
    scoring_fns: []
    benchmarks: []

    tool_groups:
      - toolgroup_id: mcp::ocr-server
        provider_id: model-context-protocol
        mcp_endpoint:
          uri: http://ocr-server.{{ include "agentic-claims-demo.namespace" . }}.svc.cluster.local:8080/sse
      - toolgroup_id: mcp::rag-server
        provider_id: model-context-protocol
        mcp_endpoint:
          uri: http://rag-server.{{ include "agentic-claims-demo.namespace" . }}.svc.cluster.local:8080/sse
      # RHOAI had:
      # - toolgroup_id: builtin::websearch
      #   provider_id: tavily-search
      - toolgroup_id: builtin::rag
        provider_id: rag-runtime

    # RHOAI didn't set this:
    # telemetry:
    #   enabled: true

    server:
      port: 8321
{{- end }}
