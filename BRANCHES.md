# Guide des Branches

## Branches Disponibles

### `main` - ReActAgent SDK (Version Actuelle)
**Utilise:** `llama-stack-client` SDK avec ReActAgent

**Code:**
```python
from llama_stack_client import LlamaStackClient
from llama_stack_client.lib.agents.agent import Agent
from llama_stack_client.lib.agents.event_logger import EventLogger

client = LlamaStackClient(base_url=settings.llamastack_endpoint)
agent = Agent(client=client, agent_config=AgentConfig(...))
response = agent.create_turn(messages=[...], session_id=session_id, stream=True)

event_logger = EventLogger()
for chunk in response:
    event_logger.log(chunk)

final_response = event_logger.get_response()
```

**Avantages:**
- âœ… Code pythonic et propre
- âœ… Type hints complets
- âœ… Gestion automatique des sessions
- âœ… EventLogger pour debugging

**InconvÃ©nients:**
- âš ï¸ Potentiel memory leak dans EventLogger (version 0.3.0rc3)
- âš ï¸ Risque d'OOM sur gros volumes
- âš ï¸ DÃ©pend de la stabilitÃ© du SDK

**Quand utiliser:**
- Pour tester la version officielle du SDK
- AprÃ¨s mise Ã  jour de llama-stack-client (>= 0.4.0)
- En dÃ©veloppement local avec monitoring mÃ©moire

---

### `http-response-api` - HTTP Direct (Version Stable)
**Utilise:** Appels HTTP directs Ã  LlamaStack

**Code:**
```python
import httpx

async with httpx.AsyncClient(timeout=120.0) as http_client:
    # Create conversation
    conv_response = await http_client.post(
        f"{settings.llamastack_endpoint}/v1/conversations",
        json={"name": f"claim_{claim_id}"}
    )
    conversation_id = conv_response.json()["conversation_id"]

    # Get response
    agent_response = await http_client.post(
        f"{settings.llamastack_endpoint}/v1/responses",
        json={
            "model": settings.llamastack_default_model,
            "conversation_id": conversation_id,
            "messages": [...],
            "tools": mcp_tools,
            "stream": False
        }
    )

    final_response = agent_response.json()["choices"][0]["message"]["content"]
```

**Avantages:**
- âœ… Pas de memory leak
- âœ… Stable et prÃ©visible
- âœ… MÃ©moire constante (~2-3GB)
- âœ… Performance: 3-4 secondes
- âœ… Facile Ã  debugger (requÃªtes brutes)

**InconvÃ©nients:**
- âš ï¸ Code plus verbose
- âš ï¸ Pas de type hints automatiques
- âš ï¸ Gestion manuelle des sessions

**Quand utiliser:**
- En production
- Si le SDK cause des OOM
- Pour performance garantie
- Pour debugging approfondi

---

## CaractÃ©ristiques Communes (Les Deux Branches)

Les deux branches incluent **toutes les corrections du vector store:**

### âœ… RAG Server OptimisÃ©
- Endpoint corrigÃ©: `/v1/vector-io/query`
- RÃ©cupÃ©ration dynamique du `vector_store_id`
- Filtrage client-side par collection
- Timing logs pour dÃ©tecter timeouts

### âœ… OCR OptimisÃ©
- Compression d'image: 70% resize + JPEG 85
- RÃ©duit le temps de traitement: 12s â†’ 8s
- Reste sous le timeout de 10s

### âœ… Vector Store Initialization
- Script `init_vectorstore.py` pour peupler LlamaStack
- Kubernetes Job pour automatisation
- GÃ©nÃ©ration d'embeddings pour knowledge_base
- Insertion via `/v1/vector-io/insert`

### âœ… Configuration LlamaStack
- Section `vector_dbs` dans le ConfigMap
- Provider pgvector configurÃ©
- Table `vector_store_llama_vectors` crÃ©Ã©e

### âœ… Scripts d'Automatisation
- `deploy-rag-and-vectorstore.sh` - DÃ©ploiement complet
- `test-rag-workflow.sh` - Tests automatisÃ©s
- `DEPLOYMENT.md` - Documentation complÃ¨te

---

## Comparaison des Performances

| MÃ©trique | ReActAgent SDK (main) | HTTP Direct (http-response-api) |
|----------|------------------------|----------------------------------|
| **Temps de traitement** | ~10s (avec EventLogger) | 3-4s |
| **MÃ©moire utilisÃ©e** | 6-8GB (avec leak) | 2-3GB |
| **StabilitÃ©** | âš ï¸ OOM possible | âœ… Stable |
| **Code** | âœ… Propre et pythonic | âš ï¸ Verbose |
| **Type hints** | âœ… Complet | âš ï¸ Manuel |
| **Debugging** | âœ… EventLogger | âœ… Logs HTTP |

---

## Changer de Branche

### Passer Ã  la version HTTP (Stable)
```bash
git checkout http-response-api

# Rebuild backend
oc start-build backend --from-dir=backend/ --follow -n claims-demo

# RedÃ©marrer
oc delete pod -l app=backend -n claims-demo
```

### Revenir Ã  la version SDK
```bash
git checkout main

# Rebuild backend
oc start-build backend --from-dir=backend/ --follow -n claims-demo

# RedÃ©marrer ET monitorer la mÃ©moire
oc delete pod -l app=backend -n claims-demo
watch -n 2 'oc adm top pod -n claims-demo -l app=backend'
```

---

## Recommandation

### Pour Production Actuelle
ðŸ‘‰ **Utiliser `http-response-api`**
- Stable et testÃ©
- Pas de risque d'OOM
- Performance garantie

### Pour DÃ©veloppement/Test
ðŸ‘‰ **Utiliser `main`**
- Tester les nouvelles versions du SDK
- Contribuer aux rapports de bugs
- Valider les fixes upstream

### AprÃ¨s Mise Ã  Jour LlamaStack
ðŸ‘‰ **Retester `main`**
- VÃ©rifier les release notes
- Tester avec monitoring mÃ©moire
- Migrer si stable

---

## Historique des ProblÃ¨mes

### 2024-12 - Memory Leak EventLogger
**Version:** llama-stack-client 0.3.0rc3
**SymptÃ´me:** OOM Exit Code 137 mÃªme avec 8GB RAM
**Cause:** EventLogger.log() accumule trop de donnÃ©es en mÃ©moire
**Solution:** Migration vers HTTP direct (branche http-response-api)

### Ã€ surveiller
- Nouvelle version de llama-stack-client (>= 0.4.0)
- Fix du memory leak dans EventLogger
- Release notes mentionnant "memory", "OOM", ou "EventLogger"

---

## Liens Utiles

- **SDK GitHub:** https://github.com/meta-llama/llama-stack-client-python
- **LlamaStack Docs:** https://llamastack.github.io/docs/
- **Guide de Test SDK:** `REACTAGENT_SDK_TESTING.md`
- **Guide DÃ©ploiement:** `DEPLOYMENT.md`
