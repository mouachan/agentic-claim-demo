apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-config-v035
  namespace: claims-demo
data:
  run.yaml: |
    version: "2"

    # Built-in providers (inline)
    built_in_providers:
      - remote::ollama           # For local Ollama models
      - inline::meta-reference   # Meta reference agents
      - remote::fireworks        # Fireworks AI
      - remote::together         # Together AI
      - inline::pgvector         # Vector DB (pgvector)
      - inline::rag-runtime      # Builtin RAG
      - remote::model-context-protocol  # MCP provider

    # Model providers
    models:
      - provider_id: vllm-inference-1
        provider_type: remote::vllm
        config:
          url: "http://llama-instruct-32-3b-predictor.edg-demo.svc.cluster.local:8000/v1"

    # MCP Tool Groups (CRITICAL for v0.3.5 - enables MCP tool execution)
    toolgroups:
      - toolgroup_id: mcp::ocr-server
        provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        mcp_endpoint:
          uri: "http://ocr-server.claims-demo.svc.cluster.local:8080/sse"

      - toolgroup_id: mcp::rag-server
        provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        mcp_endpoint:
          uri: "http://rag-server.claims-demo.svc.cluster.local:8080/sse"

    # Agents runtime
    agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config: {}

    # Vector DB for RAG (optional - if using builtin RAG)
    memory_banks:
      - provider_id: pgvector
        provider_type: inline::pgvector
        config:
          host: "postgresql.claims-demo.svc.cluster.local"
          port: 5432
          db: "claims_db"
          user: "claims_user"
          password: "${POSTGRES_PASSWORD}"
