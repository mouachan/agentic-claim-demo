# Llama Stack Configuration for Claims Demo
# Working configuration with eval provider added
version: "2"
image_name: rh

apis:
- agents
- datasetio
- files
- inference
- safety        # Required by agents
- eval          # Added
- scoring
- tool_runtime
- vector_io

providers:
  inference:
    - provider_id: sentence-transformers
      provider_type: inline::sentence-transformers
      config: {}

    - provider_id: vllm-inference-1
      provider_type: remote::vllm
      config:
        api_token: ${env.VLLM_API_TOKEN_1:=fake}
        max_tokens: ${env.VLLM_MAX_TOKENS:=16384}
        tls_verify: ${env.VLLM_TLS_VERIFY:=false}
        url: http://llama-instruct-32-3b-predictor.llama-instruct-32-3b-demo.svc.cluster.local:80/v1

    - provider_id: vllm-inference-2
      provider_type: remote::vllm
      config:
        api_token: ${env.VLLM_API_TOKEN_2:=fake}
        max_tokens: ${env.VLLM_MAX_TOKENS:=16384}
        tls_verify: ${env.VLLM_TLS_VERIFY:=false}
        url: http://mistral-3-14b-instruct-predictor.edg-demo.svc.cluster.local:80/v1

  vector_io:
    - provider_id: pgvector
      provider_type: remote::pgvector
      config:
        host: postgresql.claims-demo.svc.cluster.local
        port: 5432
        database: claims_db
        user: ${env.POSTGRES_USER:=claims_user}
        password: ${env.POSTGRES_PASSWORD:=claims_pass}
        collection_name: llama_vectors

  safety:
    # Placeholder safety provider - required by agents
    # TODO: Configure llama-guard or prompt-guard when models are available
    []

  agents:
    - provider_id: meta-reference
      provider_type: inline::meta-reference
      config:
        persistence_store:
          db_path: /opt/app-root/src/.llama/distributions/rh/agents_store.db
          namespace: null
          type: sqlite
        responses_store:
          db_path: /opt/app-root/src/.llama/distributions/rh/responses_store.db
          type: sqlite

  eval:
    - provider_id: meta-reference-eval
      provider_type: inline::meta-reference
      config:
        kvstore:
          db_path: /opt/app-root/src/.llama/distributions/rh/eval_store.db
          namespace: null
          type: sqlite

  files:
    - provider_id: meta-reference-files
      provider_type: inline::localfs
      config:
        metadata_store:
          db_path: /opt/app-root/src/.llama/distributions/rh/files_metadata.db
          type: sqlite
        storage_dir: /opt/app-root/src/.llama/distributions/rh/files

  datasetio:
    - provider_id: huggingface
      provider_type: remote::huggingface
      config:
        kvstore:
          db_path: /opt/app-root/src/.llama/distributions/rh/huggingface_datasetio.db
          namespace: null
          type: sqlite

  scoring:
    - provider_id: basic
      provider_type: inline::basic
      config: {}

    - provider_id: llm-as-judge
      provider_type: inline::llm-as-judge
      config: {}

  tool_runtime:
    - provider_id: rag-runtime
      provider_type: inline::rag-runtime
      config:
        vector_db_ids:
          - claims_vector_db

    - provider_id: model-context-protocol
      provider_type: remote::model-context-protocol
      config:
        mcp_servers:
          - name: ocr-server
            uri: sse://ocr-mcp-server.claims-demo.svc.cluster.local:8080/mcp/sse
            tools:
              - ocr_document
          - name: rag-server
            uri: sse://rag-mcp-server.claims-demo.svc.cluster.local:8080/mcp/sse
            tools:
              - retrieve_user_info
              - retrieve_similar_claims
              - search_knowledge_base

metadata_store:
  type: sqlite
  db_path: /opt/app-root/src/.llama/distributions/rh/inference_store.db

models:
  - provider_id: sentence-transformers
    model_id: granite-embedding-125m
    provider_model_id: ibm-granite/granite-embedding-125m-english
    model_type: embedding
    metadata:
      embedding_dimension: 768

  - provider_id: vllm-inference-1
    model_id: llama-instruct-32-3b
    model_type: llm
    metadata:
      description: "Llama 3.2 3B Instruct with tool calling"
      display_name: llama-instruct-32-3b

  - provider_id: vllm-inference-2
    model_id: mistral-3-14b-instruct
    model_type: llm
    metadata:
      description: "Mistral 3 14B Instruct for complex tasks"
      display_name: mistral-3-14b-instruct

shields: []

vector_dbs:
  - vector_db_id: claims_vector_db
    provider_id: pgvector
    provider_vector_db_id: claims_db
    embedding_model: granite-embedding-125m
    embedding_dimension: 768

datasets: []
scoring_fns: []
benchmarks: []

tool_groups:
  - toolgroup_id: builtin::rag
    provider_id: rag-runtime

  - toolgroup_id: claims-ocr
    provider_id: model-context-protocol
    tools:
      - name: ocr-server::ocr_document
        description: "Extract text from claim documents (PDF/images) using OCR and LLM validation"

  - toolgroup_id: claims-rag
    provider_id: model-context-protocol
    tools:
      - name: rag-server::retrieve_user_info
        description: "Retrieve user information and insurance contracts using vector search"
      - name: rag-server::retrieve_similar_claims
        description: "Find similar historical claims using vector similarity"
      - name: rag-server::search_knowledge_base
        description: "Search knowledge base for policy information with LLM synthesis"

  - toolgroup_id: claims-processing
    provider_id: model-context-protocol
    tools:
      - name: ocr-server::ocr_document
        description: "Extract text from claim documents (PDF/images) using OCR and LLM validation"
      - name: rag-server::retrieve_user_info
        description: "Retrieve user information and insurance contracts using vector search"
      - name: rag-server::retrieve_similar_claims
        description: "Find similar historical claims using vector similarity"
      - name: rag-server::search_knowledge_base
        description: "Search knowledge base for policy information with LLM synthesis"

server:
  port: 8321
