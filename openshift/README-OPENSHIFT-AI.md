# DÃ©ploiement sur OpenShift AI 3.0 - Guide Conforme

## ğŸ“š Documentation Officielle

Ce dÃ©ploiement est conforme Ã  la documentation officielle Red Hat OpenShift AI 3.0 :
- [Working with Llama Stack](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html-single/working_with_llama_stack/)
- [Deploying a RAG stack](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html/working_with_llama_stack/deploying-a-rag-stack-in-a-project_rag)

## ğŸ—ï¸ Architecture OpenShift AI 3.0

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           OpenShift AI 3.0 Cluster                   â”‚
â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Namespace: claims-demo (DataScienceCluster)   â”‚  â”‚
â”‚  â”‚                                                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ LlamaStackDistribution (CRD)             â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ - apiVersion: llamastack.io/v1alpha1     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ - kind: LlamaStackDistribution           â”‚  â”‚  â”‚
â”‚  â”‚  â”‚                                           â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ LlamaStack Server (Pod)             â”‚ â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ - Port: 8321                        â”‚ â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ - API: /inference, /embeddings      â”‚ â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚
â”‚  â”‚  â”‚           â”‚                               â”‚  â”‚  â”‚
â”‚  â”‚  â”‚           â”œâ”€â”€> vLLM Service (externe)     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚           â”‚    (vos 4 modÃ¨les LLM)        â”‚  â”‚  â”‚
â”‚  â”‚  â”‚           â”‚                               â”‚  â”‚  â”‚
â”‚  â”‚  â”‚           â””â”€â”€> Milvus/FAISS (vector DB)   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚                (inline ou remote)         â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ Serveurs MCP (Deployments standard)      â”‚  â”‚  â”‚
â”‚  â”‚  â”‚                                           â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ OCR Server      â”‚â”€â”                   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ RAG Server      â”‚â”€â”¼â”€â”€> LlamaStack    â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    Service        â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    (port 8321)    â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ Orchestrator    â”‚â”€â”˜                   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ Backend API (FastAPI)                    â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ Frontend (React)                         â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ PostgreSQL + pgvector                    â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ DiffÃ©rences ClÃ©s avec OpenShift AI 3.0

### âŒ Ce qui N'EXISTE PAS dans OpenShift AI 3.0

1. **Pas de CRD "MCPServer"**
   - Les serveurs MCP sont dÃ©ployÃ©s comme des **Deployments Kubernetes standard**

2. **Pas de CRD "LlamaStackReference"**
   - On dÃ©ploie directement **LlamaStackDistribution**
   - L'opÃ©rateur crÃ©e automatiquement le Service

3. **Pas de CRD "Guardrails" standalone**
   - Les guardrails sont implÃ©mentÃ©s dans le code applicatif
   - Pas de CRD dÃ©diÃ© dans OpenShift AI 3.0

4. **Pas de CRD "DataScienceProject"**
   - On utilise un **namespace standard** avec les labels appropriÃ©s
   - Le DataScienceCluster s'applique au niveau cluster

### âœ… Ce qui EXISTE dans OpenShift AI 3.0

1. **LlamaStackDistribution CRD**
   ```yaml
   apiVersion: llamastack.io/v1alpha1
   kind: LlamaStackDistribution
   ```

2. **OpÃ©rateur LlamaStack**
   - GÃ¨re automatiquement le dÃ©ploiement
   - RÃ©sout l'image `rh-dev` vers le bon registry
   - CrÃ©e le Service automatiquement sur port 8321

3. **IntÃ©gration vLLM**
   - LlamaStack se connecte Ã  vos modÃ¨les vLLM existants
   - Via variables d'environnement `VLLM_URL`, `INFERENCE_MODEL`

4. **Vector Stores intÃ©grÃ©s**
   - **Milvus inline** : DÃ©ployÃ© automatiquement avec LlamaStack
   - **Milvus remote** : Connexion Ã  un Milvus existant
   - **FAISS inline** : Plus simple, avec SQLite backend

## ğŸ“ PrÃ©requis avant DÃ©ploiement

### 1. Identifier vos ModÃ¨les LLM Existants

```bash
# Lister les services vLLM
oc get svc -A | grep vllm

# Exemple de sortie :
# ai-models    vllm-llama-3-2-3b     ClusterIP   10.0.1.100   <none>   8000/TCP
# ai-models    vllm-llama-3-2-70b    ClusterIP   10.0.1.101   <none>   8000/TCP
# ai-models    vllm-mistral-7b       ClusterIP   10.0.1.102   <none>   8000/TCP
# ai-models    vllm-embedding-model  ClusterIP   10.0.1.103   <none>   8000/TCP
```

### 2. VÃ©rifier l'OpÃ©rateur LlamaStack

```bash
# VÃ©rifier que l'opÃ©rateur est installÃ©
oc get csv -n openshift-operators | grep llamastack

# VÃ©rifier les CRDs disponibles
oc get crd | grep llamastack
```

### 3. Activer GPU Support (si nÃ©cessaire)

Selon la doc Red Hat, pour utiliser des modÃ¨les LLM avec GPU :
```bash
# Installer NVIDIA GPU Operator
# (Suivre la doc OpenShift GPU support)
```

## ğŸš€ DÃ©ploiement Ã‰tape par Ã‰tape

### Phase 1 : CrÃ©er le Namespace

```bash
# CrÃ©er le namespace pour la dÃ©mo
oc new-project claims-demo

# Ajouter les labels appropriÃ©s
oc label namespace claims-demo \
  opendatahub.io/dashboard=true \
  modelmesh-enabled=true
```

### Phase 2 : CrÃ©er les Secrets

```bash
cd openshift/secrets

# Secret pour vLLM API token (si nÃ©cessaire)
oc create secret generic vllm-api-token \
  --from-literal=token='YOUR_VLLM_API_TOKEN' \
  -n claims-demo

# Secret pour PostgreSQL
oc create secret generic postgresql-secret \
  --from-literal=POSTGRES_USER=claims_user \
  --from-literal=POSTGRES_PASSWORD='GÃ‰NÃ‰RER_UN_MOT_DE_PASSE_FORT' \
  -n claims-demo

# Secret pour Milvus (si remote)
oc create secret generic milvus-credentials \
  --from-literal=username=milvus_user \
  --from-literal=password='MILVUS_PASSWORD' \
  -n claims-demo
```

### Phase 3 : DÃ©ployer LlamaStackDistribution

**Avant d'appliquer, modifiez `openshift/crds/llamastack-distribution.yaml` :**

1. Remplacez `VLLM_URL` par l'URL de votre service vLLM :
   ```yaml
   - name: VLLM_URL
     value: "http://vllm-llama-3-2.<namespace>.svc.cluster.local:8000"
   ```

2. Choisissez votre configuration de vector store :
   - **Option A** : Milvus inline (recommandÃ© pour dÃ©marrer)
   - **Option B** : Milvus remote (si vous avez dÃ©jÃ  Milvus)
   - **Option C** : FAISS inline (plus simple, pas de serveur externe)

3. Adaptez les noms de modÃ¨les selon vos 4 modÃ¨les :
   ```yaml
   - name: INFERENCE_MODEL
     value: "meta-llama/Llama-3.2-3b-instruct"  # Votre modÃ¨le
   ```

**Puis dÃ©ployez :**

```bash
cd openshift/crds

# DÃ©ployer LlamaStack avec Milvus inline (recommandÃ©)
oc apply -f llamastack-distribution.yaml -n claims-demo

# Attendre que le pod soit prÃªt
oc wait --for=condition=Ready pod -l app=llama-stack -n claims-demo --timeout=600s

# VÃ©rifier le dÃ©ploiement
oc get llamastackdistribution -n claims-demo
oc get pods -l app=llama-stack -n claims-demo
oc logs -f deployment/claims-llamastack -n claims-demo
```

### Phase 4 : VÃ©rifier le Service LlamaStack

```bash
# Le service est crÃ©Ã© automatiquement par l'opÃ©rateur
oc get svc -n claims-demo | grep llama

# Exemple de sortie :
# claims-llamastack   ClusterIP   10.0.2.50   <none>   8321/TCP

# Tester l'endpoint (depuis un pod debug)
oc run test-pod --rm -it --image=curlimages/curl -- \
  curl http://claims-llamastack.claims-demo.svc.cluster.local:8321/health
```

### Phase 5 : DÃ©ployer PostgreSQL

```bash
cd openshift

# Appliquer le PVC
oc apply -f pvcs/postgresql-pvc.yaml -n claims-demo

# Appliquer le StatefulSet
oc apply -f deployments/postgresql-statefulset.yaml -n claims-demo

# Appliquer le Service
oc apply -f services/postgresql-service.yaml -n claims-demo

# Attendre que PostgreSQL soit prÃªt
oc wait --for=condition=Ready pod -l app=postgresql -n claims-demo --timeout=300s

# Initialiser la base
POD=$(oc get pod -l app=postgresql -n claims-demo -o jsonpath='{.items[0].metadata.name}')
oc cp database/init.sql $POD:/tmp/init.sql -n claims-demo
oc exec -it $POD -n claims-demo -- psql -U claims_user -d claims_db -f /tmp/init.sql

# Charger les donnÃ©es de test
oc cp database/seed_data/001_sample_data.sql $POD:/tmp/seed.sql -n claims-demo
oc exec -it $POD -n claims-demo -- psql -U claims_user -d claims_db -f /tmp/seed.sql
```

### Phase 6 : DÃ©ployer les Serveurs MCP

Les serveurs MCP sont dÃ©ployÃ©s comme des **Deployments Kubernetes standard**, pas des CRDs.

**Configuration de la variable d'environnement** :
```yaml
env:
  - name: LLAMASTACK_ENDPOINT
    value: "http://claims-llamastack.claims-demo.svc.cluster.local:8321"
```

```bash
cd openshift/deployments

# OCR Server
oc apply -f ocr-server-deployment.yaml -n claims-demo
oc apply -f ../services/ocr-server-service.yaml -n claims-demo

# RAG Server
oc apply -f rag-server-deployment.yaml -n claims-demo
oc apply -f ../services/rag-server-service.yaml -n claims-demo

# Orchestrator Server
oc apply -f orchestrator-server-deployment.yaml -n claims-demo
oc apply -f ../services/orchestrator-server-service.yaml -n claims-demo

# VÃ©rifier les dÃ©ploiements
oc get pods -n claims-demo | grep -E "ocr|rag|orchestrator"
```

### Phase 7 : DÃ©ployer Backend et Frontend

```bash
# Backend
oc apply -f backend-deployment.yaml -n claims-demo
oc apply -f ../services/backend-service.yaml -n claims-demo
oc apply -f ../routes/backend-route.yaml -n claims-demo

# Frontend
oc apply -f frontend-deployment.yaml -n claims-demo
oc apply -f ../services/frontend-service.yaml -n claims-demo
oc apply -f ../routes/frontend-route.yaml -n claims-demo

# RÃ©cupÃ©rer les URLs
FRONTEND_URL=$(oc get route frontend -n claims-demo -o jsonpath='{.spec.host}')
BACKEND_URL=$(oc get route backend -n claims-demo -o jsonpath='{.spec.host}')

echo "Frontend: https://$FRONTEND_URL"
echo "Backend API: https://$BACKEND_URL"
```

## ğŸ§ª Tests

### Test 1 : VÃ©rifier LlamaStack

```bash
# Test depuis un pod
oc run test --rm -it --image=curlimages/curl -n claims-demo -- \
  curl http://claims-llamastack:8321/models

# Devrait retourner la liste des modÃ¨les
```

### Test 2 : VÃ©rifier les MCP Servers

```bash
# OCR Server
oc run test --rm -it --image=curlimages/curl -n claims-demo -- \
  curl http://ocr-server:8080/health

# RAG Server
oc run test --rm -it --image=curlimages/curl -n claims-demo -- \
  curl http://rag-server:8080/health

# Orchestrator
oc run test --rm -it --image=curlimages/curl -n claims-demo -- \
  curl http://orchestrator-server:8080/health
```

### Test 3 : Workflow Complet

```bash
# Depuis votre navigateur
open https://$FRONTEND_URL

# Suivre les logs en temps rÃ©el
oc logs -f deployment/backend -n claims-demo
oc logs -f deployment/ocr-server -n claims-demo
oc logs -f deployment/rag-server -n claims-demo
oc logs -f deployment/orchestrator-server -n claims-demo
```

## ğŸ“Š Monitoring

```bash
# VÃ©rifier le statut de tous les pods
oc get pods -n claims-demo

# VÃ©rifier les ressources LlamaStack
oc describe llamastackdistribution claims-llamastack -n claims-demo

# Logs LlamaStack
oc logs deployment/claims-llamastack -n claims-demo

# MÃ©triques (si Prometheus activÃ©)
oc get servicemonitor -n claims-demo
```

## ğŸ”§ Troubleshooting

### LlamaStack ne dÃ©marre pas

```bash
# VÃ©rifier les events
oc get events -n claims-demo --sort-by='.lastTimestamp'

# VÃ©rifier les logs de l'opÃ©rateur
oc logs -n openshift-operators deployment/llamastack-operator

# VÃ©rifier la configuration vLLM
oc describe llamastackdistribution claims-llamastack -n claims-demo
```

### Serveurs MCP ne peuvent pas atteindre LlamaStack

```bash
# Tester la rÃ©solution DNS
oc run test --rm -it --image=busybox -n claims-demo -- \
  nslookup claims-llamastack.claims-demo.svc.cluster.local

# Tester la connectivitÃ©
oc run test --rm -it --image=curlimages/curl -n claims-demo -- \
  curl -v http://claims-llamastack:8321/health
```

## ğŸ“š Sources

- [Working with Llama Stack - OpenShift AI 3.0](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html-single/working_with_llama_stack/)
- [Deploying a RAG stack](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html/working_with_llama_stack/deploying-a-rag-stack-in-a-project_rag)
- [Configuring OAuth authentication](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html/working_with_llama_stack/auth-on-llama-stack_rag)
